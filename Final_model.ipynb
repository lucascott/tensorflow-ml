{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = './data/'\n",
    "\n",
    "ATT_FILE = PREFIX + \"MedianHouseValuePreparedCleanAttributes.csv\"\n",
    "LABEL_FILE = PREFIX + \"MedianHouseValueOneHotEncodedClasses.csv\"\n",
    "\n",
    "train_ratio = 0.8\n",
    "\n",
    "attributes = pd.read_csv(ATT_FILE)\n",
    "labels = pd.read_csv(LABEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train, develop and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, t_train, t_test = train_test_split(attributes, labels, test_size=1 - train_ratio, stratify=labels)\n",
    "x_dev, x_test, t_dev, t_test = train_test_split(x_test, t_test, test_size=0.5, stratify=t_test)\n",
    "\n",
    "print(\"x_train:\", x_train.shape)\n",
    "print(\"t_train:\", t_train.shape)\n",
    "\n",
    "print(\"x_dev:\", x_dev.shape)\n",
    "print(\"t_dev:\", t_dev.shape)\n",
    "\n",
    "print(\"x_test:\", x_test.shape)\n",
    "print(\"t_test:\", t_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data shape future implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUTS = x_train.shape[1]\n",
    "OUTPUTS = t_train.shape[1]\n",
    "NUM_TRAINING_EXAMPLES = x_train.shape[0]\n",
    "NUM_DEV_EXAMPLES = x_dev.shape[0]\n",
    "NUM_TEST_EXAMPLES = x_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define number of epochs and batch size for the training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 500\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining placeholder for our data, labels and the drop out layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(dtype=tf.float32, shape=(None, INPUTS), name=\"X\")\n",
    "t = tf.placeholder(dtype=tf.float32, shape=(None, OUTPUTS), name=\"t\")\n",
    "# Define dropout placeholder\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "dropout_rate = 0.2  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = INPUTS\n",
    "# Define neurons per layer\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_hidden3 = 50\n",
    "# 10 different classes\n",
    "n_outputs = OUTPUTS\n",
    "# Activation function\n",
    "activation = tf.nn.elu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    # We are goingt o use HE INITIALIZATION\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    # Utilizamos el método de Tensorflow tf.layers.dense ==> crea una red totalmemte conectada \n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1,kernel_initializer=he_init, activation=activation, name=\"hidden1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, 0.1, training=training)\n",
    "\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, name=\"hidden2\", activation=activation)\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, 0.4, training=training)\n",
    "\n",
    "    hidden3 = tf.layers.dense(hidden2_drop, n_hidden3, name=\"hidden3\", activation=activation)\n",
    "    hidden3_drop = tf.layers.dropout(hidden3, 0.1, training=training)\n",
    "\n",
    "    # Logits es el output de la red neuronal ANTES de pasar por la softmax activation function.\n",
    "    logits = tf.layers.dense(hidden3_drop, OUTPUTS, name=\"outputs\")\n",
    "    logits_summary = tf.summary.scalar('logits', logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss/cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"cost\"):\n",
    "    y = tf.nn.softmax(logits=logits, name=\"y\")\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=t, logits=logits)\n",
    "    loss = tf.reduce_mean(cross_entropy, name=\"cost\")\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optmizer and learning rate scheduler (Exponential decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    initial_learning_rate = 0.05\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/20\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                               decay_steps, decay_rate)\n",
    "                                               \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) #--> 0.35567516\n",
    "\n",
    "    train_step = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct_predictions = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "    correct_summary = tf.summary.scalar('correct', tf.cast(correct_predictions, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create initial node and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model path configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = log_dir(\"DNN_MEDIAN_HOUES\")\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "checkpoint_path = \"/tmp/DNN_profiles.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"/Users/robertollopcardenal/Desktop/developer/python/ml/deeplearning/assignment-1-Median house/tensorflow-ml/models/DNN_profiles.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop out and mini batch configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = np.infty\n",
    "epochs_without_progress = 0\n",
    "max_epochs_without_progress = 50\n",
    "minibatch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        for j in range(len(x_train) // minibatch_size):\n",
    "            sess.run(train_step,\n",
    "                     feed_dict={X: x_train[j * minibatch_size:(j + 1) * minibatch_size],\n",
    "                                t: t_train[j * minibatch_size:(j + 1) * minibatch_size]\n",
    "                                })\n",
    "            accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run([accuracy, loss, accuracy_summary, loss_summary], feed_dict={X: x_dev, t: t_dev})\n",
    "            file_writer.add_summary(accuracy_summary_str, epoch)\n",
    "            file_writer.add_summary(loss_summary_str, epoch)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch:\", epoch,\n",
    "                  \"\\tValidation accuracy: {:.3f}%\".format(accuracy_val * 100),\n",
    "                  \"\\tLoss: {:.5f}\".format(loss_val))\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "            # Si el error es menor que el anterior seguimos entrenando\n",
    "            if loss_val < best_loss:\n",
    "                saver.save(sess, final_model_path)\n",
    "                best_loss = loss_val\n",
    "            else:\n",
    "                # Sumamos de 5 en 5, porque estamos dentro del IF \n",
    "                epochs_without_progress += 5\n",
    "                # Si después de m\"ax_epochs_without_progress\", el error no ha disminuido \"OVERFITTING\" ==> EARLY STOPPING\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "            accuracy_test = accuracy.eval(feed_dict={X: x_test, t: t_test})\n",
    "            test_predictions = y.eval(feed_dict={X: x_test})\n",
    "            test_correct_predictions = correct_predictions.eval(\n",
    "            feed_dict={X: x_test, t: t_test})\n",
    "\n",
    "file_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
